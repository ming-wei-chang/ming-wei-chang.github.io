---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

#layout: home
layout: default
---


My name is Ming-Wei Chang. I am currently a Research Scientist at Google Research, working on machine learning and natural language processing problems.

## Selected Projects ([Google Scholar](https://scholar.google.com/citations?user=GiCqMFkAAAAJ&hl=en&oi=ao))

Here are some of fun projects I have worked on.

### **BERT**. [paper](https://arxiv.org/abs/1810.04805)

 BERT is a framework for pre-training deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art results for 11 nlp tasks when it was published.

### Load forecasting using SVM. [paper](https://ieeexplore.ieee.org/abstract/document/1350819/)

One of my first research projects. Using SVM to predict the power needed to balance the supply and load for powerplants. Winner of the EUNITE competition 2001.

### Zero-shot Entity linking. [paper](https://arxiv.org/abs/1906.07348)

The power of text understanding makes zero-shot entity linking finally possible. 

### Semantic parsing for knownledge base. [paper](https://www.microsoft.com/en-us/research/publication/semantic-parsing-via-staged-query-graph-generation-question-answering-with-knowledge-base/)

   By applying an advanced entity linking [system](https://arxiv.org/abs/1609.08075) and a deep convolutional neural network model that matches questions and predicate sequences, this system outperforms previous methods substantially when it was published.

### Semantic parsing using weak supervision. [paper](https://www.aclweb.org/anthology/W10-2903.pdf)

   Shows that learning with a weak feedback signal is capable of producing strong semantic parsers for the first time.

